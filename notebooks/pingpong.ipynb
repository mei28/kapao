{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29222902-dea7-4bf2-a0e0-4547e85ed982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "FILE = Path().resolve()\n",
    "\n",
    "import argparse\n",
    "from pytube import YouTube\n",
    "import os.path as osp\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "from utils.general import check_img_size\n",
    "from utils.datasets import LoadImages\n",
    "from models.experimental import attempt_load\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from val import run_nms, post_process_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea110613-5090-4a53-bb8b-bc58ad123601",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data', type=str, default='data/coco-kp.yaml')\n",
    "parser.add_argument('--imgsz', type=int, default=1280)\n",
    "parser.add_argument('--weights', default='kapao_s_coco.pt')\n",
    "parser.add_argument('--device', default='', help='cuda device, i.e. 0 or cpu')\n",
    "parser.add_argument('--half', action='store_true')\n",
    "parser.add_argument('--conf-thres', type=float, default=0.5, help='confidence threshold')\n",
    "parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
    "parser.add_argument('--no-kp-dets', action='store_true', help='do not use keypoint objects')\n",
    "parser.add_argument('--conf-thres-kp', type=float, default=0.5)\n",
    "parser.add_argument('--conf-thres-kp-person', type=float, default=0.2)\n",
    "parser.add_argument('--iou-thres-kp', type=float, default=0.45)\n",
    "parser.add_argument('--overwrite-tol', type=int, default=50)\n",
    "parser.add_argument('--scales', type=float, nargs='+', default=[1])\n",
    "parser.add_argument('--flips', type=int, nargs='+', default=[-1])\n",
    "parser.add_argument('--display', action='store_true', help='display inference results')\n",
    "parser.add_argument('--fps', action='store_true', help='display fps')\n",
    "parser.add_argument('--gif', action='store_true', help='create fig')\n",
    "parser.add_argument('--start', type=int, default=20, help='start time (s)')\n",
    "parser.add_argument('--end', type=int, default=80, help='end time (s)')\n",
    "args = parser.parse_args(args=[\"--start\",\"0\",\"--end\",\"150\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d769912-a562-439a-82e8-3cd3879acb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.data) as f:\n",
    "    data = yaml.safe_load(f)  # load data dict\n",
    "\n",
    "# add inference settings to data dict\n",
    "data['imgsz'] = args.imgsz\n",
    "data['conf_thres'] = args.conf_thres\n",
    "data['iou_thres'] = args.iou_thres\n",
    "data['use_kp_dets'] = not args.no_kp_dets\n",
    "data['conf_thres_kp'] = args.conf_thres_kp\n",
    "data['iou_thres_kp'] = args.iou_thres_kp\n",
    "data['conf_thres_kp_person'] = args.conf_thres_kp_person\n",
    "data['overwrite_tol'] = args.overwrite_tol\n",
    "data['scales'] = args.scales\n",
    "data['flips'] = [None if f == -1 else f for f in args.flips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa9c848-6e86-4a0c-b63d-f3d9db1545a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_NAME = 'movie/mp4/twitter_pingpong.mp4'\n",
    "assert osp.isfile(VIDEO_NAME)\n",
    "\n",
    "\n",
    "GRAY = (200, 200, 200)\n",
    "CROWD_THRES = 450  # max bbox size for crowd classification\n",
    "CROWD_ALPHA = 0.5\n",
    "CROWD_KP_SIZE = 2\n",
    "CROWD_KP_THICK = 2\n",
    "CROWD_SEG_THICK = 2\n",
    "\n",
    "BLUE = (245, 140, 66)\n",
    "ORANGE = (66, 140, 245)\n",
    "PLAYER_ALPHA_BOX = 0.85\n",
    "PLAYER_ALPHA_POSE = 0.3\n",
    "PLAYER_KP_SIZE = 4\n",
    "PLAYER_KP_THICK = 4\n",
    "PLAYER_SEG_THICK = 4\n",
    "FPS_TEXT_SIZE = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f97c9d-1f4d-4be0-87fc-fda50d409324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = select_device(args.device, batch_size=1)\n",
    "print('Using device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d29c7c-f633-49c9-96f2-f85dbda75adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yang/.pyenv/versions/3.8.6/envs/kapao/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = attempt_load(args.weights, map_location=device)  # load FP32 model\n",
    "half = args.half & (device.type != 'cpu')\n",
    "if half:  # half precision only supported on CUDA\n",
    "    model.half()\n",
    "stride = int(model.stride.max())  # model stride\n",
    "\n",
    "imgsz = check_img_size(args.imgsz, s=stride)  # check image size\n",
    "dataset = LoadImages('./{}'.format(VIDEO_NAME), img_size=imgsz, stride=stride, auto=True)\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d2cfd3-a29b-4153-89af-8584f8d7621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = dataset.cap\n",
    "cap.set(cv2.CAP_PROP_POS_MSEC, args.start * 1000)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "n = int(fps * (args.end - args.start))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "gif_frames = []\n",
    "video_name = 'pingpong_inference_{}'.format(osp.splitext(args.weights)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db5b1e38-8908-4905-a1e8-ff1072ec16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (path, img, im0,_) in enumerate(dataset):\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img = img / 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None]  # expand for batch dim\n",
    "\n",
    "    out = model(img, augment=True, kp_flip=data['kp_flip'], scales=data['scales'], flips=data['flips'])[0]\n",
    "    person_dets, kp_dets = run_nms(data, out)\n",
    "    bboxes, poses, scores, ids, fused = post_process_batch(data, img, [], [[im0.shape[:2]]], person_dets, kp_dets)\n",
    "\n",
    "    bboxes = np.array(bboxes)\n",
    "    poses = np.array(poses)\n",
    "\n",
    "    im0_copy = im0.copy()\n",
    "    player_idx = []    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3b38d6a-a6de-4d33-a898-5f1acd8c4bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     986.26,      237.57,           0],\n",
       "       [     985.31,       235.2,           0],\n",
       "       [     987.52,      235.33,           0],\n",
       "       [     981.55,      235.79,           0],\n",
       "       [     993.35,      235.92,           0],\n",
       "       [     973.82,      245.88,     0.51308],\n",
       "       [     997.62,      245.84,     0.52574],\n",
       "       [     967.34,      263.12,           0],\n",
       "       [     1008.5,      263.64,           0],\n",
       "       [        971,      262.65,           0],\n",
       "       [     1006.3,      264.44,           0],\n",
       "       [     979.05,      279.72,           0],\n",
       "       [     997.17,      279.85,           0],\n",
       "       [     967.95,      295.76,           0],\n",
       "       [     992.81,      296.69,           0],\n",
       "       [     966.89,      312.46,           0],\n",
       "       [      992.4,      313.26,           0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f71a8-0719-4708-b4ef-3d2746f1f585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
