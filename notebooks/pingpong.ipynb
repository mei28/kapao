{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29222902-dea7-4bf2-a0e0-4547e85ed982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "FILE = Path().resolve()\n",
    "import os \n",
    "os.chdir(str(FILE.parent))\n",
    "\n",
    "sys.path.append(str(FILE.parent))\n",
    "import argparse\n",
    "from pytube import YouTube\n",
    "import os.path as osp\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "from utils.general import check_img_size\n",
    "from utils.datasets import LoadImages\n",
    "from models.experimental import attempt_load\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from val import run_nms, post_process_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea110613-5090-4a53-bb8b-bc58ad123601",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data', type=str, default='data/coco-kp.yaml')\n",
    "parser.add_argument('--imgsz', type=int, default=1280)\n",
    "parser.add_argument('--weights', default='kapao_s_coco.pt')\n",
    "parser.add_argument('--device', default='', help='cuda device, i.e. 0 or cpu')\n",
    "parser.add_argument('--half', action='store_true')\n",
    "parser.add_argument('--conf-thres', type=float, default=0.5, help='confidence threshold')\n",
    "parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
    "parser.add_argument('--no-kp-dets', action='store_true', help='do not use keypoint objects')\n",
    "parser.add_argument('--conf-thres-kp', type=float, default=0.5)\n",
    "parser.add_argument('--conf-thres-kp-person', type=float, default=0.2)\n",
    "parser.add_argument('--iou-thres-kp', type=float, default=0.45)\n",
    "parser.add_argument('--overwrite-tol', type=int, default=50)\n",
    "parser.add_argument('--scales', type=float, nargs='+', default=[1])\n",
    "parser.add_argument('--flips', type=int, nargs='+', default=[-1])\n",
    "parser.add_argument('--display', action='store_true', help='display inference results')\n",
    "parser.add_argument('--fps', action='store_true', help='display fps')\n",
    "parser.add_argument('--gif', action='store_true', help='create fig')\n",
    "parser.add_argument('--start', type=int, default=20, help='start time (s)')\n",
    "parser.add_argument('--end', type=int, default=80, help='end time (s)')\n",
    "args = parser.parse_args(args=[\"--start\",\"0\",\"--end\",\"120\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d769912-a562-439a-82e8-3cd3879acb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.data) as f:\n",
    "    data = yaml.safe_load(f)  # load data dict\n",
    "\n",
    "# add inference settings to data dict\n",
    "data['imgsz'] = args.imgsz\n",
    "data['conf_thres'] = args.conf_thres\n",
    "data['iou_thres'] = args.iou_thres\n",
    "data['use_kp_dets'] = not args.no_kp_dets\n",
    "data['conf_thres_kp'] = args.conf_thres_kp\n",
    "data['iou_thres_kp'] = args.iou_thres_kp\n",
    "data['conf_thres_kp_person'] = args.conf_thres_kp_person\n",
    "data['overwrite_tol'] = args.overwrite_tol\n",
    "data['scales'] = args.scales\n",
    "data['flips'] = [None if f == -1 else f for f in args.flips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa9c848-6e86-4a0c-b63d-f3d9db1545a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_NAME = 'movie/mp4/twitter_pingpong.mp4'\n",
    "video_name = 'twitter_pingpong.mp4'\n",
    "assert osp.isfile(VIDEO_NAME)\n",
    "\n",
    "\n",
    "GRAY = (200, 200, 200)\n",
    "CROWD_THRES = 200  # max bbox size for crowd classification\n",
    "CROWD_ALPHA = 0.5\n",
    "CROWD_KP_SIZE = 2\n",
    "CROWD_KP_THICK = 2\n",
    "CROWD_SEG_THICK = 2\n",
    "\n",
    "BLUE = (245, 140, 66)\n",
    "ORANGE = (66, 140, 245)\n",
    "PLAYER_ALPHA_BOX = 0.85\n",
    "PLAYER_ALPHA_POSE = 0.3\n",
    "PLAYER_KP_SIZE = 4\n",
    "PLAYER_KP_THICK = 4\n",
    "PLAYER_SEG_THICK = 4\n",
    "FPS_TEXT_SIZE = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f97c9d-1f4d-4be0-87fc-fda50d409324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = select_device(args.device, batch_size=1)\n",
    "print('Using device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d29c7c-f633-49c9-96f2-f85dbda75adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mei/.pyenv/versions/3.8.6/envs/kapao/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Writing inference video:   0%| | 0/3600 [00:00<?, ?"
     ]
    }
   ],
   "source": [
    "model = attempt_load(args.weights, map_location=device)  # load FP32 model\n",
    "half = args.half & (device.type != 'cpu')\n",
    "if half:  # half precision only supported on CUDA\n",
    "    model.half()\n",
    "stride = int(model.stride.max())  # model stride\n",
    "\n",
    "imgsz = check_img_size(args.imgsz, s=stride)  # check image size\n",
    "dataset = LoadImages('./{}'.format(VIDEO_NAME), img_size=imgsz, stride=stride, auto=True)\n",
    "\n",
    "cap = dataset.cap\n",
    "cap.set(cv2.CAP_PROP_POS_MSEC, args.start * 1000)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "n = int(fps * (args.end - args.start))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "gif_frames = []\n",
    "video_name = 'pingpong_inference_{}'.format(osp.splitext(args.weights)[0])\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "if not args.display:\n",
    "    writer = cv2.VideoWriter(video_name + '.mp4',\n",
    "                             cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "    if not args.fps:  # tqdm might slows down inference\n",
    "        dataset = tqdm(dataset, desc='Writing inference video', total=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a80d3b-17ba-48ce-bd18-462f2b89e972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing inference video: 3697it [01:14, 49.49it/s] \n"
     ]
    }
   ],
   "source": [
    "# bboxを取得\n",
    "t0 = time_sync()\n",
    "\n",
    "output_bboxes = []\n",
    "output_fused = []\n",
    "output_scores = []\n",
    "\n",
    "for i, (path, img, im0,_) in enumerate(dataset):\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img = img / 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None]  # expand for batch dim\n",
    "\n",
    "    out = model(img, augment=True, kp_flip=data['kp_flip'], scales=data['scales'], flips=data['flips'])[0]\n",
    "    person_dets, kp_dets = run_nms(data, out)\n",
    "    bboxes, poses, scores, ids, fused = post_process_batch(data, img, [], [[im0.shape[:2]]], person_dets, kp_dets)\n",
    "\n",
    "    bboxes = np.array(bboxes)\n",
    "    poses = np.array(poses)\n",
    "    scores = np.array(scores)\n",
    "    fused = np.array(fused)\n",
    "    \n",
    "   \n",
    "    output_bboxes.append(bboxes)\n",
    "    output_scores.append(scores)\n",
    "    output_fused.append(fused)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34d94016-2e00-4c0e-b314-6f38de4d5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tldr2ltwh(bboxes,scores):\n",
    "    \"\"\"convert topleft downright to left top width hight\n",
    "    Args:\n",
    "        bboxes (list): shape = [frames,nums,[x1 y1 x2 y2]]\n",
    "    \n",
    "    Returns:\n",
    "        list: [<frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <x>, <y>, <z>]\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for i, (bbox,score) in enumerate(zip(bboxes,scores),start=1):\n",
    "        for (x1, y1, x2, y2),(conf) in zip(bbox,score):\n",
    "            bb_left = x1\n",
    "            bb_top = y1\n",
    "            bb_width = abs(x2-x1)\n",
    "            bb_height = abs(y1-y2)\n",
    "            \n",
    "            _box = [i,-1,bb_left,bb_top,bb_width,bb_height,conf,-1,-1,-1]\n",
    "        \n",
    "            ret.append(_box)\n",
    "    return ret\n",
    "ltwh_bboxes = convert_tldr2ltwh(output_bboxes,output_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca76b70b-4080-4780-9b7a-7a3a367fa6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame(ltwh_bboxes)\n",
    "\n",
    "npy = np.array(ltwh_bboxes)\n",
    "np.save('detection.npy',npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43239504-39b7-4b84-8fde-4f8efdbbd552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('kapao_detections.pickle', mode='wb') as f:\n",
    "    pickle.dump(ltwh_bboxes,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5b1e38-8908-4905-a1e8-ff1072ec16da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# t0 = time_sync()\n",
    "# for i, (path, img, im0,_) in enumerate(dataset):\n",
    "#     img = torch.from_numpy(img).to(device)\n",
    "#     img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "#     img = img / 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "#     if len(img.shape) == 3:\n",
    "#         img = img[None]  # expand for batch dim\n",
    "\n",
    "#     out = model(img, augment=True, kp_flip=data['kp_flip'], scales=data['scales'], flips=data['flips'])[0]\n",
    "#     person_dets, kp_dets = run_nms(data, out)\n",
    "#     bboxes, poses, scores, ids, fused = post_process_batch(data, img, [], [[im0.shape[:2]]], person_dets, kp_dets)\n",
    "\n",
    "#     bboxes = np.array(bboxes)\n",
    "#     poses = np.array(poses)\n",
    "\n",
    "#     im0_copy = im0.copy()\n",
    "#     player_idx = []   \n",
    "#     # DRAW CROWD POSES\n",
    "#     for j, (bbox, pose) in enumerate(zip(bboxes, poses)):\n",
    "#         x1, y1, x2, y2 = bbox\n",
    "#         size = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n",
    "#         if size < CROWD_THRES:\n",
    "#             cv2.rectangle(im0_copy, (int(x1), int(y1)), (int(x2), int(y2)), GRAY, thickness=2)\n",
    "#             for x, y, _ in pose[:5]:\n",
    "#                 cv2.circle(im0_copy, (int(x), int(y)), CROWD_KP_SIZE, GRAY, CROWD_KP_THICK)\n",
    "#             for seg in data['segments'].values():\n",
    "#                 pt1 = (int(pose[seg[0], 0]), int(pose[seg[0], 1]))\n",
    "#                 pt2 = (int(pose[seg[1], 0]), int(pose[seg[1], 1]))\n",
    "#                 cv2.line(im0_copy, pt1, pt2, GRAY, CROWD_SEG_THICK)\n",
    "#         else:\n",
    "#             player_idx.append(j)\n",
    "#     im0 = cv2.addWeighted(im0, CROWD_ALPHA, im0_copy, 1 - CROWD_ALPHA, gamma=0)\n",
    "#     # DRAW PLAYER POSES\n",
    "#     player_bboxes = bboxes[player_idx][:2]\n",
    "#     player_poses = poses[player_idx][:2]\n",
    "    \n",
    "    \n",
    "\n",
    "#     def draw_player_poses(im0, missing=-1):\n",
    "#         for j, (bbox, pose, color) in enumerate(zip(\n",
    "#                 player_bboxes[[orange_player, blue_player]],\n",
    "#                 player_poses[[orange_player, blue_player]],\n",
    "#                 [ORANGE, BLUE])):\n",
    "#             if j == missing:\n",
    "#                 continue\n",
    "#             im0_copy = im0.copy()\n",
    "#             x1, y1, x2, y2 = bbox\n",
    "#             cv2.rectangle(im0_copy, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness=-1)\n",
    "#             im0 = cv2.addWeighted(im0, PLAYER_ALPHA_BOX, im0_copy, 1 - PLAYER_ALPHA_BOX, gamma=0)\n",
    "#             im0_copy = im0.copy()\n",
    "#             for x, y, _ in pose:\n",
    "#                 cv2.circle(im0_copy, (int(x), int(y)), PLAYER_KP_SIZE, color, PLAYER_KP_THICK)\n",
    "#             for seg in data['segments'].values():\n",
    "#                 pt1 = (int(pose[seg[0], 0]), int(pose[seg[0], 1]))\n",
    "#                 pt2 = (int(pose[seg[1], 0]), int(pose[seg[1], 1]))\n",
    "#                 cv2.line(im0_copy, pt1, pt2, color, PLAYER_SEG_THICK)\n",
    "#             im0 = cv2.addWeighted(im0, PLAYER_ALPHA_POSE, im0_copy, 1 - PLAYER_ALPHA_POSE, gamma=0)\n",
    "#         return im0   \n",
    "\n",
    "#     if i == 0:\n",
    "#         # orange player on left at start\n",
    "#         orange_player = np.argmin(player_bboxes[:, 0])\n",
    "#         blue_player = int(not orange_player)\n",
    "#         im0 = draw_player_poses(im0)\n",
    "#     else:\n",
    "#         # simple player tracking based on frame-to-frame pose difference\n",
    "#         dist = []\n",
    "#         for pose in poses_last:\n",
    "#             dist.append(np.mean(np.linalg.norm(player_poses[0, :, :2] - pose[:, :2], axis=-1)))\n",
    "#         if np.argmin(dist) == 0:\n",
    "#             orange_player = 0\n",
    "#         else:\n",
    "#             orange_player = 1\n",
    "#         blue_player = int(not orange_player)\n",
    "\n",
    "#         # if only one player detected, find which player is missing\n",
    "#         missing = -1\n",
    "#         if len(player_poses) == 1:\n",
    "#             if orange_player == 0:  # missing blue player\n",
    "#                 player_poses = np.concatenate((player_poses, poses_last[1:]), axis=0)\n",
    "#                 player_bboxes = np.concatenate((player_bboxes, bboxes_last[1:]), axis=0)\n",
    "#                 missing = 1\n",
    "#             else:  # missing orange player\n",
    "#                 player_poses = np.concatenate((player_poses, poses_last[:1]), axis=0)\n",
    "#                 player_bboxes = np.concatenate((player_bboxes, bboxes_last[:1]), axis=0)\n",
    "#                 missing = 0\n",
    "#         im0 = draw_player_poses(im0, missing)\n",
    "\n",
    "#     bboxes_last = player_bboxes[[orange_player, blue_player]]\n",
    "#     poses_last = player_poses[[orange_player, blue_player]]\n",
    "\n",
    "#     if i == 0:\n",
    "#         t = time_sync() - t0\n",
    "#     else:\n",
    "#         t = time_sync() - t1\n",
    "#     if args.gif:\n",
    "#         gif_frames.append(cv2.resize(im0, dsize=None, fx=0.25, fy=0.25)[:, :, [2, 1, 0]])\n",
    "#     elif not args.display:\n",
    "#         writer.write(im0)\n",
    "#     else:\n",
    "#         cv2.imshow('', cv2.resize(im0, dsize=None, fx=0.5, fy=0.5))\n",
    "#         cv2.waitKey(1)\n",
    "\n",
    "#     t1 = time_sync()\n",
    "#     if i == n - 1:\n",
    "#         break\n",
    "    \n",
    "\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "# cap.release()\n",
    "# if not args.display:\n",
    "#     writer.release()\n",
    "# if args.gif:\n",
    "#     print('Saving GIF...')\n",
    "#     with imageio.get_writer(video_name + '.gif', mode=\"I\", fps=fps) as writer:\n",
    "#         for idx, frame in tqdm(enumerate(gif_frames)):\n",
    "#             writer.append_data(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92046924-65ee-4984-9f96-e619ae78ed59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'player_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_578878/4184315889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayer_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'player_idx' is not defined"
     ]
    }
   ],
   "source": [
    "player_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a7c2b-b607-4846-93c2-c521f986eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da3964-ecb1-4490-a05f-c858b36eec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
