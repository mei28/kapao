{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29222902-dea7-4bf2-a0e0-4547e85ed982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "FILE = Path().resolve()\n",
    "import os \n",
    "os.chdir(str(FILE.parent))\n",
    "\n",
    "sys.path.append(str(FILE.parent))\n",
    "import argparse\n",
    "from pytube import YouTube\n",
    "import os.path as osp\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "from utils.general import check_img_size\n",
    "from utils.datasets import LoadImages\n",
    "from models.experimental import attempt_load\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import yaml\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import imageio\n",
    "from val import run_nms, post_process_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea110613-5090-4a53-bb8b-bc58ad123601",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data', type=str, default='data/coco-kp.yaml')\n",
    "parser.add_argument('--imgsz', type=int, default=1280)\n",
    "parser.add_argument('--weights', default='kapao_s_coco.pt')\n",
    "parser.add_argument('--device', default='', help='cuda device, i.e. 0 or cpu')\n",
    "parser.add_argument('--half', action='store_true')\n",
    "parser.add_argument('--conf-thres', type=float, default=0.5, help='confidence threshold')\n",
    "parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
    "parser.add_argument('--no-kp-dets', action='store_true', help='do not use keypoint objects')\n",
    "parser.add_argument('--conf-thres-kp', type=float, default=0.5)\n",
    "parser.add_argument('--conf-thres-kp-person', type=float, default=0.2)\n",
    "parser.add_argument('--iou-thres-kp', type=float, default=0.45)\n",
    "parser.add_argument('--overwrite-tol', type=int, default=50)\n",
    "parser.add_argument('--scales', type=float, nargs='+', default=[1])\n",
    "parser.add_argument('--flips', type=int, nargs='+', default=[-1])\n",
    "parser.add_argument('--display', action='store_true', help='display inference results')\n",
    "parser.add_argument('--fps', action='store_true', help='display fps')\n",
    "parser.add_argument('--gif', action='store_true', help='create fig')\n",
    "parser.add_argument('--start', type=int, default=20, help='start time (s)')\n",
    "parser.add_argument('--end', type=int, default=80, help='end time (s)')\n",
    "args = parser.parse_args(args=[\"--start\",\"0\",\"--end\",\"120\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d769912-a562-439a-82e8-3cd3879acb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.data) as f:\n",
    "    data = yaml.safe_load(f)  # load data dict\n",
    "\n",
    "# add inference settings to data dict\n",
    "data['imgsz'] = args.imgsz\n",
    "data['conf_thres'] = args.conf_thres\n",
    "data['iou_thres'] = args.iou_thres\n",
    "data['use_kp_dets'] = not args.no_kp_dets\n",
    "data['conf_thres_kp'] = args.conf_thres_kp\n",
    "data['iou_thres_kp'] = args.iou_thres_kp\n",
    "data['conf_thres_kp_person'] = args.conf_thres_kp_person\n",
    "data['overwrite_tol'] = args.overwrite_tol\n",
    "data['scales'] = args.scales\n",
    "data['flips'] = [None if f == -1 else f for f in args.flips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa9c848-6e86-4a0c-b63d-f3d9db1545a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_NAME = 'movie/mp4/twitter_pingpong.mp4'\n",
    "video_name = 'twitter_pingpong.mp4'\n",
    "assert osp.isfile(VIDEO_NAME)\n",
    "\n",
    "\n",
    "GRAY = (200, 200, 200)\n",
    "CROWD_THRES = 200  # max bbox size for crowd classification\n",
    "CROWD_ALPHA = 0.5\n",
    "CROWD_KP_SIZE = 2\n",
    "CROWD_KP_THICK = 2\n",
    "CROWD_SEG_THICK = 2\n",
    "\n",
    "BLUE = (245, 140, 66)\n",
    "ORANGE = (66, 140, 245)\n",
    "PLAYER_ALPHA_BOX = 0.85\n",
    "PLAYER_ALPHA_POSE = 0.3\n",
    "PLAYER_KP_SIZE = 4\n",
    "PLAYER_KP_THICK = 4\n",
    "PLAYER_SEG_THICK = 4\n",
    "FPS_TEXT_SIZE = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f97c9d-1f4d-4be0-87fc-fda50d409324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = select_device(args.device, batch_size=1)\n",
    "print('Using device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13d29c7c-f633-49c9-96f2-f85dbda75adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attempt_load(args.weights, map_location=device)  # load FP32 model\n",
    "half = args.half & (device.type != 'cpu')\n",
    "if half:  # half precision only supported on CUDA\n",
    "    model.half()\n",
    "stride = int(model.stride.max())  # model stride\n",
    "\n",
    "imgsz = check_img_size(args.imgsz, s=stride)  # check image size\n",
    "dataset = LoadImages('./{}'.format(VIDEO_NAME), img_size=imgsz, stride=stride, auto=True)\n",
    "\n",
    "cap = dataset.cap\n",
    "cap.set(cv2.CAP_PROP_POS_MSEC, args.start * 1000)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "n = int(fps * (args.end - args.start))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "gif_frames = []\n",
    "video_name = 'pingpong_inference_{}'.format(osp.splitext(args.weights)[0])\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "if not args.display:\n",
    "    writer = cv2.VideoWriter(video_name + '.mp4',cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "    if not args.fps:  # tqdm might slows down inference\n",
    "        progress_dataset = (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8a80d3b-17ba-48ce-bd18-462f2b89e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bboxを取得\n",
    "t0 = time_sync()\n",
    "\n",
    "output_bboxes = []\n",
    "output_poses = []\n",
    "output_fused = []\n",
    "output_scores = []\n",
    "\n",
    "for i, (path, img, im0,_) in enumerate(progress_dataset):\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img = img / 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None]  # expand for batch dim\n",
    "\n",
    "    out = model(img, augment=True, kp_flip=data['kp_flip'], scales=data['scales'], flips=data['flips'])[0]\n",
    "    person_dets, kp_dets = run_nms(data, out)\n",
    "    bboxes, poses, scores, ids, fused = post_process_batch(data, img, [], [[im0.shape[:2]]], person_dets, kp_dets)\n",
    "\n",
    "    bboxes = np.array(bboxes)\n",
    "    poses = np.array(poses)\n",
    "    scores = np.array(scores)\n",
    "    fused = np.array(fused)\n",
    "    \n",
    "   \n",
    "    output_bboxes.append(bboxes)\n",
    "    output_poses.append(poses)\n",
    "    output_scores.append(scores)\n",
    "    output_fused.append(fused)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34d94016-2e00-4c0e-b314-6f38de4d5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tldr2ltwh(bboxes,scores)->list:\n",
    "    \"\"\"convert topleft downright to left top width hight\n",
    "    Args:\n",
    "        bboxes (list): shape = [frames,nums,[x1 y1 x2 y2]]\n",
    "    \n",
    "    Returns:\n",
    "        list: [<frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <x>, <y>, <z>]\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for i, (bbox,score) in enumerate(zip(bboxes,scores),start=1):\n",
    "        for (x1, y1, x2, y2),(conf) in zip(bbox,score):\n",
    "            bb_left = x1\n",
    "            bb_top = y1\n",
    "            bb_width = abs(x2-x1)\n",
    "            bb_height = abs(y1-y2)\n",
    "            \n",
    "            _box = [i,-1,bb_left,bb_top,bb_width,bb_height,conf,-1,-1,-1]\n",
    "        \n",
    "            ret.append(_box)\n",
    "    return ret\n",
    "\n",
    "def convert_ltwh2tldr(bboxes)->list:\n",
    "    \"\"\" convert left top width height to topleft downright\n",
    "    Args:\n",
    "        bboxes (list):         \n",
    "            list: [<frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <x>, <y>, <z>]\n",
    "\n",
    "    Returns:\n",
    "        list: [<frame>, <id>, <x1>, <y1>, <x2>, <y2>, <conf>, <x>, <y>, <z>]\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for row in bboxes:\n",
    "        x2 = row[2] + row[4]\n",
    "        y2 = row[3] + row[5]\n",
    "        _row = [row[0],row[1],row[2],row[3],x2,y2,row[6],row[7],row[8],row[9]]\n",
    "        assert len(row) == len(_row)\n",
    "        del row\n",
    "        ret.append(_row)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def convert_poses_MOTformat(poses)->list:\n",
    "    \"\"\" convert format from kapao to MOT-like\n",
    "    Args:\n",
    "        poses: [frame,person,poses]\n",
    "    \n",
    "    Returns:\n",
    "        list: [<frame>, <id>, <poses>(np.ndarray)]\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    \n",
    "    for i,frame_block in enumerate(poses,start=1):\n",
    "        # 一フレームごと\n",
    "        for j, _pose in enumerate(frame_block):\n",
    "            ret.append([i,j, _pose])\n",
    "    return ret \n",
    "\n",
    "def get_masked_data(data:list, mask_id:int=-1):\n",
    "    \"\"\"get data whose mask id is the same.\"\"\"\n",
    "    ret = []\n",
    "    for d in data:\n",
    "        if d[0]==mask_id:\n",
    "            ret.append(d)\n",
    "        if d[0]>mask_id:\n",
    "            break\n",
    "    return ret\n",
    "\n",
    "def get_max_frame(data:list)->int:\n",
    "    \"\"\" get max frame number\n",
    "    Args:\n",
    "        data (list): MOT-like format\n",
    "    \n",
    "    Returns:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return data[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "721a0804-8af9-4d3c-b75e-f4d2af6926a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltwh_bboxes = convert_tldr2ltwh(output_bboxes,output_scores)\n",
    "tldr_bboxes = convert_ltwh2tldr(ltwh_bboxes)\n",
    "mot_poses  = convert_poses_MOTformat(output_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b467a4de-c816-451f-a675-c50ca68ad00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, -1, 281.0, 327.0, 449.0, 615.0, 0.9247974, -1, -1, -1],\n",
       " [1, -1, 724.0, 279.0, 948.0, 498.0, 0.87222314, -1, -1, -1],\n",
       " [1, -1, 1033.0, 225.0, 1126.0, 574.0, 0.8538191, -1, -1, -1],\n",
       " [1, -1, 149.0, 289.0, 208.0, 376.0, 0.7752365, -1, -1, -1],\n",
       " [1, -1, 57.0, 292.0, 120.0, 372.0, 0.6448778, -1, -1, -1],\n",
       " [1, -1, 960.0, 225.0, 1015.0, 310.0, 0.600439, -1, -1, -1],\n",
       " [1, -1, 1145.0, 231.0, 1172.0, 260.0, 0.59739846, -1, -1, -1]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[1100, -1, 12.0, 287.0, 268.0, 658.0, 0.9105787, -1, -1, -1],\n",
       " [1100, -1, 1023.0, 221.0, 1113.0, 574.0, 0.8576677, -1, -1, -1],\n",
       " [1100, -1, 708.0, 253.0, 891.0, 478.0, 0.854373, -1, -1, -1],\n",
       " [1100, -1, 463.0, 229.0, 508.0, 311.0, 0.62112606, -1, -1, -1],\n",
       " [1100, -1, 1143.0, 231.0, 1170.0, 263.0, 0.5603369, -1, -1, -1]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(get_masked_data(tldr_bboxes, 1))\n",
    "display(get_masked_data(tldr_bboxes, 1100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a03b9d2-8a09-48c6-b742-6194e2ae40be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52257133a72743578c7411b6a98d60bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = LoadImages('./{}'.format(VIDEO_NAME), img_size=imgsz, stride=stride, auto=True)\n",
    "dataset = tqdm(dataset,total=n)\n",
    "t0 = time_sync()\n",
    "\n",
    "for frame_id , (path,img,im0,_) in enumerate(dataset,start=1):\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half()\n",
    "    img /= 255.0\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None]\n",
    "    \n",
    "    im0_copy = im0.copy()\n",
    "    \n",
    "    # 対象のフレームとして切り出した\n",
    "    mask_poses = get_masked_data(mot_poses,frame_id)\n",
    "    mask_bboxes = get_masked_data(tldr_bboxes,frame_id)\n",
    "    \n",
    "    for _poses, _bboxes in zip(mask_poses,mask_bboxes):\n",
    "        x1,y1,x2,y2 = _bboxes[2:6]\n",
    "        cv2.rectangle(im0_copy,(int(x1),int(y1)),(int(x2),int(y2)),GRAY, thickness=2)\n",
    "    im0 = cv2.addWeighted(im0, CROWD_ALPHA, im0_copy, 1 - CROWD_ALPHA, gamma=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    if frame_id == 1:\n",
    "        t = time_sync() - t0\n",
    "    else:\n",
    "        t = time_sync() - t1\n",
    "    if args.gif:\n",
    "        gif_frames.append(cv2.resize(im0, dsize=None, fx=0.25, fy=0.25)[:, :, [2, 1, 0]])\n",
    "    elif not args.display:\n",
    "        writer.write(im0)\n",
    "    else:\n",
    "        cv2.imshow('', cv2.resize(im0, dsize=None, fx=0.5, fy=0.5))\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    t1 = time_sync()\n",
    "    if i == n - 1:\n",
    "        break\n",
    "\n",
    "\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "if not args.display:\n",
    "    writer.release()\n",
    "if args.gif:\n",
    "    print('Saving GIF...')\n",
    "    with imageio.get_writer(video_name + '.gif', mode=\"I\", fps=fps) as writer:\n",
    "        for idx, frame in tqdm(enumerate(gif_frames)):\n",
    "            writer.append_data(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b1e38-8908-4905-a1e8-ff1072ec16da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# t0 = time_sync()\n",
    "# for i, (path, img, im0,_) in enumerate(progress_dataset):\n",
    "#     img = torch.from_numpy(img).to(device)\n",
    "#     img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "#     img = img / 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "#     if len(img.shape) == 3:\n",
    "#         img = img[None]  # expand for batch dim\n",
    "\n",
    "#     out = model(img, augment=True, kp_flip=data['kp_flip'], scales=data['scales'], flips=data['flips'])[0]\n",
    "#     person_dets, kp_dets = run_nms(data, out)\n",
    "#     bboxes, poses, scores, ids, fused = post_process_batch(data, img, [], [[im0.shape[:2]]], person_dets, kp_dets)\n",
    "\n",
    "#     bboxes = np.array(bboxes)\n",
    "#     poses = np.array(poses)\n",
    "\n",
    "#     im0_copy = im0.copy()\n",
    "#     player_idx = []   \n",
    "#     # DRAW CROWD POSES\n",
    "#     for j, (bbox, pose) in enumerate(zip(bboxes, poses)):\n",
    "#         x1, y1, x2, y2 = bbox\n",
    "#         size = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n",
    "#         if size < CROWD_THRES:\n",
    "#             cv2.rectangle(im0_copy, (int(x1), int(y1)), (int(x2), int(y2)), GRAY, thickness=2)\n",
    "#             for x, y, _ in pose[:5]:\n",
    "#                 cv2.circle(im0_copy, (int(x), int(y)), CROWD_KP_SIZE, GRAY, CROWD_KP_THICK)\n",
    "#             for seg in data['segments'].values():\n",
    "#                 pt1 = (int(pose[seg[0], 0]), int(pose[seg[0], 1]))\n",
    "#                 pt2 = (int(pose[seg[1], 0]), int(pose[seg[1], 1]))\n",
    "#                 cv2.line(im0_copy, pt1, pt2, GRAY, CROWD_SEG_THICK)\n",
    "#         else:\n",
    "#             player_idx.append(j)\n",
    "#     im0 = cv2.addWeighted(im0, CROWD_ALPHA, im0_copy, 1 - CROWD_ALPHA, gamma=0)\n",
    "#     # DRAW PLAYER POSES\n",
    "#     player_bboxes = bboxes[player_idx][:2]\n",
    "#     player_poses = poses[player_idx][:2]\n",
    "    \n",
    "    \n",
    "\n",
    "#     def draw_player_poses(im0, missing=-1):\n",
    "#         for j, (bbox, pose, color) in enumerate(zip(\n",
    "#                 player_bboxes[[orange_player, blue_player]],\n",
    "#                 player_poses[[orange_player, blue_player]],\n",
    "#                 [ORANGE, BLUE])):\n",
    "#             if j == missing:\n",
    "#                 continue\n",
    "#             im0_copy = im0.copy()\n",
    "#             x1, y1, x2, y2 = bbox\n",
    "#             cv2.rectangle(im0_copy, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness=-1)\n",
    "#             im0 = cv2.addWeighted(im0, PLAYER_ALPHA_BOX, im0_copy, 1 - PLAYER_ALPHA_BOX, gamma=0)\n",
    "#             im0_copy = im0.copy()\n",
    "#             for x, y, _ in pose:\n",
    "#                 cv2.circle(im0_copy, (int(x), int(y)), PLAYER_KP_SIZE, color, PLAYER_KP_THICK)\n",
    "#             for seg in data['segments'].values():\n",
    "#                 pt1 = (int(pose[seg[0], 0]), int(pose[seg[0], 1]))\n",
    "#                 pt2 = (int(pose[seg[1], 0]), int(pose[seg[1], 1]))\n",
    "#                 cv2.line(im0_copy, pt1, pt2, color, PLAYER_SEG_THICK)\n",
    "#             im0 = cv2.addWeighted(im0, PLAYER_ALPHA_POSE, im0_copy, 1 - PLAYER_ALPHA_POSE, gamma=0)\n",
    "#         return im0   \n",
    "\n",
    "#     if i == 0:\n",
    "#         # orange player on left at start\n",
    "#         orange_player = np.argmin(player_bboxes[:, 0])\n",
    "#         blue_player = int(not orange_player)\n",
    "#         im0 = draw_player_poses(im0)\n",
    "#     else:\n",
    "#         # simple player tracking based on frame-to-frame pose difference\n",
    "#         dist = []\n",
    "#         for pose in poses_last:\n",
    "#             dist.append(np.mean(np.linalg.norm(player_poses[0, :, :2] - pose[:, :2], axis=-1)))\n",
    "#         if np.argmin(dist) == 0:\n",
    "#             orange_player = 0\n",
    "#         else:\n",
    "#             orange_player = 1\n",
    "#         blue_player = int(not orange_player)\n",
    "\n",
    "#         # if only one player detected, find which player is missing\n",
    "#         missing = -1\n",
    "#         if len(player_poses) == 1:\n",
    "#             if orange_player == 0:  # missing blue player\n",
    "#                 player_poses = np.concatenate((player_poses, poses_last[1:]), axis=0)\n",
    "#                 player_bboxes = np.concatenate((player_bboxes, bboxes_last[1:]), axis=0)\n",
    "#                 missing = 1\n",
    "#             else:  # missing orange player\n",
    "#                 player_poses = np.concatenate((player_poses, poses_last[:1]), axis=0)\n",
    "#                 player_bboxes = np.concatenate((player_bboxes, bboxes_last[:1]), axis=0)\n",
    "#                 missing = 0\n",
    "#         im0 = draw_player_poses(im0, missing)\n",
    "\n",
    "#     bboxes_last = player_bboxes[[orange_player, blue_player]]\n",
    "#     poses_last = player_poses[[orange_player, blue_player]]\n",
    "\n",
    "#     if i == 0:\n",
    "#         t = time_sync() - t0\n",
    "#     else:\n",
    "#         t = time_sync() - t1\n",
    "#     if args.gif:\n",
    "#         gif_frames.append(cv2.resize(im0, dsize=None, fx=0.25, fy=0.25)[:, :, [2, 1, 0]])\n",
    "#     elif not args.display:\n",
    "#         writer.write(im0)\n",
    "#     else:\n",
    "#         cv2.imshow('', cv2.resize(im0, dsize=None, fx=0.5, fy=0.5))\n",
    "#         cv2.waitKey(1)\n",
    "\n",
    "#     t1 = time_sync()\n",
    "#     if i == n - 1:\n",
    "#         break\n",
    "    \n",
    "\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "# cap.release()\n",
    "# if not args.display:\n",
    "#     writer.release()\n",
    "# if args.gif:\n",
    "#     print('Saving GIF...')\n",
    "#     with imageio.get_writer(video_name + '.gif', mode=\"I\", fps=fps) as writer:\n",
    "#         for idx, frame in tqdm(enumerate(gif_frames)):\n",
    "#             writer.append_data(frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
